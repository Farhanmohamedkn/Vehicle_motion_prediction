{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21752b6c-10ec-4964-b0ae-f29804abf1f7",
   "metadata": {},
   "source": [
    "# Example Motion Prediction\n",
    "In this example, we take a look at how to perform motion prediction in python in a Jupyter environment. We will walk you through the steps and hopefully, you will be able to modify this example. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f34356-9e42-493e-a6f9-45c97f990c43",
   "metadata": {},
   "source": [
    "## 0.0 Importing \n",
    "In python you have to tell the script which modules you want to use. [Help](https://docs.python.org/3/reference/import.html) \n",
    "Each \"module\" consists of \"functions\" that can be used after importing the module. You can also `import` single functions `from` a module. For later use, you can assign a nickname to a module, like `numpy as np`. You will call `numpy` functions as `np.someFunction` then. \n",
    "Oh, by the way: This text you are reading right now is written in  [Markdown](https://en.wikipedia.org/wiki/Markdown) format. Which comes with a bunch of formatting features as you can see. This can help to communicate your thoughts in such a notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc268928-8ef1-449c-844b-f27f3b66db46",
   "metadata": {},
   "source": [
    "### 0.1 Importing Modules\n",
    "Since we are working in Jupyter, we can run the cell below with all the import commands independently of the code below that cell.\n",
    "Click into the cell below and do one of the things below:\n",
    "- `Shift + Enter` will run the cell and moves to the next cell.\n",
    "- `Crtl + Enter` will run the cell but won't move to the next cell.\n",
    "- Move your mouse to the play symbol above and click."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c39e371",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully imported!!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import xlsxwriter\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Succesfully imported!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83e2368",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "I also included a `print` command, so the notebook tells me right in place, that the cell is successfully evaluated, which is quite neat!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cb3f60",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 0.2 Adding Local Libraries\n",
    "For this example, we've coded up some stuff to make our lifes (and maybe your's) easier. But since we didn't want to have all this arkward code hanging around in this nice little notebook, we crammed it into some regular python scripts, which can also be used in Jupyter. To take a look at this code, follow the path below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13a29790",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append('data_processing/')\n",
    "from readDataset import dataGrabber\n",
    "from preProcessing import preProcess\n",
    "from dataPreparation import dataPrepare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff888e4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1.0 Get the Dataset Path\n",
    "To start working, we need some data. If you put the dataset to the right place, the following command will point to the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4054cdf",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/data/\n"
     ]
    }
   ],
   "source": [
    "dataset_path = './dataset/data/'\n",
    "print(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c77534b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2.0 Reading Data \n",
    "***(Use either 2.1 or 2.2)***\n",
    "\n",
    "Our helper function are able to read the dataset from the csv-files and introduce it into the Jupyter workspace. Please use **one of the following two options!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ea102b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.1 Data reading with location ID\n",
    "Taking a look into the dataset, it is organized in locations with several recordings. The following will load all recordings from one location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b2876ca",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Edit to some other number to load a different dataset\n",
    "locations_sel = ['2']\n",
    "\n",
    "# Initialize data Grabber Object\n",
    "data_obj = dataGrabber(dataset_path)\n",
    "\n",
    "data_obj.location_id = locations_sel\n",
    "data_obj.read_csv_with_location()\n",
    "\n",
    "track_data_raw = data_obj.get_tracks_data()\n",
    "track_meta_data_raw = data_obj.get_tracksMeta_data()\n",
    "# data_obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96216a0e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "Now we created a `data_obj`, which is an \"object\". In python you can use the whole array of \"object-oriented-programming\" [(OOP)](https://en.wikipedia.org/wiki/Object-oriented_programming) concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17856605",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2.2 Data reading with recording ID\n",
    "Alternatively, you can use the following cell to load several recordings by recording ID. The code below is \"commented out\", so if you run the cell the python interpreter doesn't read it as instructions, but as text (that usually should comment what the code does).\n",
    "If you remove the `\"\"\"` above and below the code, you can \"uncomment\" it and use it as acutal code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9aacd20",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (1285584737.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\MOHAMED FARHAN KN\\AppData\\Local\\Temp\\ipykernel_25592\\1285584737.py\"\u001b[1;36m, line \u001b[1;32m12\u001b[0m\n\u001b[1;33m    \"\"\"\"\u001b[0m\n\u001b[1;37m        \n^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "recording_id_sel = ['22']\n",
    "\n",
    "# Initialize data Grabber Object\n",
    "data_obj = dataGrabber(dataset_path)\n",
    "\n",
    "data_obj.recording_id = recording_id_sel\n",
    "data_obj.read_csv_with_recordingID()\n",
    "\n",
    "track_data_raw = data_obj.get_tracks_data()\n",
    "track_meta_data_raw = data_obj.get_tracksMeta_data()\n",
    "\"\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f6b1a7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.0 Preprocessing the Data\n",
    "When working with data, it occurs that we get weird, noisy, unnecessary, large, unstructured ... chunks of excel sheets. From this starting point, it is our job to make use of this data and turn it into information.\n",
    "In our case, the dataset is at least in a given order, but we need to bring it into a nice structure to work with it. Therefore we create a `pre_process_obj` and hand it the data to extract some info first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22cda410",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pre_process_obj = preProcess()\n",
    "pre_process_obj.tracks_data = track_data_raw\n",
    "pre_process_obj.tracks_meta_data = track_meta_data_raw\n",
    "pre_process_obj.recording_ids = data_obj.recording_id\n",
    "pre_process_obj.data_len = len(track_data_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce06dc7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.1 Downsampling Data\n",
    "\n",
    "Take look at the recording files and how many MB they consume on your hard drive. Now check how much RAM your machine has. To juggle all this data constantly on your RAM might be impossible for your machine (most likely) or it is possible and you should overthink your priorities in life.\n",
    "Nonetheless, you don't need all your data to make nice prediction! More often than not, it is totally sufficient to use every $k$-th piece of data. The important thing to keep in mind is that you shouldn't make important **information** or **characteristics** of your data disappear. How do you know when this happens? You don't. At least you do not know for general data. For a specific subclass there exists the famous [Nyquistâ€“Shannon sampling theorem](https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem). Maybe you can lend some intuition of that idea to get the picture right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc27709-7dee-4937-9da5-82c4e3d58ad5",
   "metadata": {},
   "source": [
    "See how many pieces of data you should skip to fit the requirements. Besides that, play around with the volume of data:\n",
    "- What's the minimum frame rate you can get away with to make a nice prediction?\n",
    "- What's the maximum frame rate your PC can handle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11dc6292",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define the number of frames to be skipped + 1 => here 4 frames are skipped so 4+1 = 5\n",
    "\n",
    "pre_process_obj.frames_skipped = 5\n",
    "track_data_downsampled, tracks_meta_data = pre_process_obj.get_down_sampled_data()\n",
    "pre_process_obj.tracks_data = track_data_downsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01355f4f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     1,     2, ..., 33133, 33138, 33143], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.unique(track_data_downsampled[\"frame\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb64e8c1",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set 'True' or 'False'  ->> Avoids unnecessary data preparation while physics based predition\n",
    "PHYSICS_BASED_PREDICTION = False \n",
    "DATA_DRIVEN_PREDICTION = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef8df3c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.2 For Physical Model Based Prediction\n",
    "No more cleaning is required because our specific dataset already looks kind of clean. At least in our view and for this example. Maybe you are up to something that requires additional steps. \n",
    "\n",
    "Some things you could encounter in data are\n",
    "1. Missing data: Some track could be lost for some time and could be found again. How do you treat this gap? Did the dataset assign a new track ID although it is the same physical vehicle? \n",
    "2. Numeric issues: Everybody who uses google maps knows that teleportation is a thing. When you measure stuff, you can get weird outliers (huge or very small numbers) or for some reason they can get so huge that your PC cannot assign a numeric value anymore and therefore calls it NaN (\"Not a Number\"). \"NaN\"s are troublesome to handle because they are no numbers unlike all the regular, benign, well-behaved numbers in your data series. Some functions you want to use simply don't except NaNs and throw an error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63593d2b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if PHYSICS_BASED_PREDICTION:\n",
    "    track_data_downsamp_phy_model = track_data_downsampled \n",
    "    track_meta_data_phy_model = tracks_meta_data\n",
    "### So maybe you want to jump directly to section 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f8aa65",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.3 For Data-Driven Prediction Models\n",
    "\n",
    "The following steps are specific to data-driven prediction models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3c9bd9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.3.1 Label Encoding\n",
    "Since the dataset contains alphabetical datatypes we need to cast them into numerical datatypes, since we want to process it mathematically. A common way to achieve this, is to \"encode\" \"classes\" as \"integer\" values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f02a583a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels Encoded\n",
      "0 : bicycle\n",
      "1 : car\n",
      "2 : pedestrian\n",
      "3 : truck_bus\n"
     ]
    }
   ],
   "source": [
    "if DATA_DRIVEN_PREDICTION:\n",
    "    pre_process_obj.label_encoding()\n",
    "    pre_process_obj.print_label_encoder_classes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beb09ca",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### 3.3.2 Normalizing data between 0 and 1\n",
    "To understand this step, I want you to refer to my youtube channel, where we've put together a nice playlist, with some background information about machine learning, motion prediction, python, jupyter notebooks and all that good stuff. Watch the linked video first!\n",
    "\n",
    "Smash that like-button, subscribe and ring the bell and so on...!\n",
    "\n",
    "[![](pictures_main/scalingYouTube.png)](https://www.youtube.com/watch?v=Y7m9MyPxcyQ&list=PLZ6Q6guexAFO_gbbxjxrNVrFx0zgIbmyN&index=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19519aea",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## \"min_max_scalar_list\" order:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac06f045",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```\n",
    "Useful for inverse transform\n",
    "        0: 'xCenter',   1: 'yCenter',       2: 'heading',       3: 'width',       4: 'length', 5: 'xVelocity',\n",
    "        6: 'yVelocity', 7: 'xAcceleration', 8: 'yAcceleration', 9: 'lonVelocity', 10: 'latVelocity', \n",
    "       11:'lonAcceleration',  \n",
    "       12:'latAcceleration'\n",
    "           To USE: min_max_scalar_list[0] .... etc \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "938a1f81",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    " if DATA_DRIVEN_PREDICTION:\n",
    "    # Gets the tracks data normalized\n",
    "    tracks_data_norm, min_max_scalar_list = pre_process_obj.normalize_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de49e989",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## 4.0 Preparing Data for Training and Testing\n",
    "Our prediction function should receive some chunks of data from the past $x$ and should predict the output $y$. From the dataset, we know which $y$ should be predicted based on each $x$. So at time step $t_i$ we need to know our current data point $x_i$ (could be position or velocity or whatever) and some history of data points $x_{i-1} \\dots x_{i-N_{past}}\\,$  from the time steps before. ![](pictures_main/slidingWindow.png)\n",
    "*Fig: Sliding Window on a time series*\n",
    "\n",
    "When training a neural network, we want to approximate a function $f( \\,)$ that maps our inputs $x$ to our outputs $y$\n",
    "$$\\large{y} = \\large{f} \\big(\n",
    "        \\begin{bmatrix}\n",
    "            \\mathbf{x_1}^T  & \n",
    "            \\mathbf{x_2}^T & \n",
    "            \\mathbf{\\dots} &\n",
    "            \\mathbf{x_n}^T &\n",
    "        \\end{bmatrix}\n",
    "    \\big)$$\n",
    "    \n",
    "[![](pictures_main/NeuralNetworkYouTube.png)](https://www.youtube.com/watch?v=aircAruvnKk&list=PLZ6Q6guexAFO_gbbxjxrNVrFx0zgIbmyN&index=2)\n",
    "    \n",
    "Therefore, we first need to know how our predictions $y$ should look like, so our training process is able to know how well the desired function is approximated. So in this case, we also have to grab the future values $x_{i+1} \\dots x_{i+N_{future}}\\,$ to declare them as output values $y_{i+1} \\dots y_{i+N_{future}}\\,$.\n",
    "![](pictures_main/slidingWindow_Y.png)\n",
    "*Fig: Sliding Window with inputs and outputs with $N_{past}$ = 3, $N_{future}$ = 2*\n",
    "\n",
    "We need to do that anyway, because in the end, we need to evaluate how well we did with our prediction and therefore we will predict $\\hat{y}_{i+1} \\dots \\hat{y}_{i+N_{future}}\\,$ and need to compare this with the actual $y_{i+1} \\dots y_{i+N_{future}}\\,$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d26186f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.1 Loading Data\n",
    "First, we hand all the necessary data about the data to the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59726820",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if DATA_DRIVEN_PREDICTION:\n",
    "    # Resetting dropped frames index\n",
    "    tracks_data_norm = tracks_data_norm.reset_index(drop=True)\n",
    "    \n",
    "    data_prepare_obj = dataPrepare()\n",
    "    data_prepare_obj.tracks_data_norm = tracks_data_norm\n",
    "    data_prepare_obj.tracksMeta_data = tracks_meta_data\n",
    "    data_prepare_obj.data_len = len(tracks_data_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2bd2f9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.2 Splitting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d273f4b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "With its function `data_stacking()` the `data_prepare_obj` grabs the vectors from our time series in this sliding window fashion. Additionally, it stacks all the obtained sequences underneath each other in a single vector. Also additionally, it splits the final huge vector into a set of \"training\" data and \"validation\" data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3ecf6c4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This might take a while!\n",
      "Current progress: 0.0 %\n",
      "Current progress: 1.01 %\n",
      "Current progress: 2.02 %\n",
      "Current progress: 3.03 %\n",
      "Current progress: 4.04 %\n",
      "Current progress: 5.05 %\n",
      "Current progress: 6.06 %\n",
      "Current progress: 7.07 %\n",
      "Current progress: 8.08 %\n",
      "Current progress: 9.09 %\n",
      "Current progress: 10.1 %\n",
      "Current progress: 11.11 %\n",
      "Current progress: 12.12 %\n",
      "Current progress: 13.13 %\n",
      "Current progress: 14.14 %\n",
      "Current progress: 15.15 %\n",
      "Current progress: 16.16 %\n",
      "Current progress: 17.17 %\n",
      "Current progress: 18.18 %\n",
      "Current progress: 19.19 %\n",
      "Current progress: 20.2 %\n",
      "Current progress: 21.21 %\n",
      "Current progress: 22.22 %\n",
      "Current progress: 23.23 %\n",
      "Current progress: 24.24 %\n",
      "Current progress: 25.25 %\n",
      "Current progress: 26.26 %\n",
      "Current progress: 27.27 %\n",
      "Current progress: 28.28 %\n",
      "Current progress: 29.29 %\n",
      "Current progress: 30.3 %\n",
      "Current progress: 31.31 %\n",
      "Current progress: 32.32 %\n",
      "Current progress: 33.33 %\n",
      "Current progress: 34.34 %\n",
      "Current progress: 35.35 %\n",
      "Current progress: 36.36 %\n",
      "Current progress: 37.37 %\n",
      "Current progress: 38.38 %\n",
      "Current progress: 39.39 %\n",
      "Current progress: 40.4 %\n",
      "Current progress: 41.41 %\n",
      "Current progress: 42.42 %\n",
      "Current progress: 43.43 %\n",
      "Current progress: 44.44 %\n",
      "Current progress: 45.45 %\n",
      "Current progress: 46.46 %\n",
      "Current progress: 47.47 %\n",
      "Current progress: 48.48 %\n",
      "Current progress: 49.49 %\n",
      "Current progress: 50.51 %\n",
      "Current progress: 51.52 %\n",
      "Current progress: 52.53 %\n",
      "Current progress: 53.54 %\n",
      "Current progress: 54.55 %\n",
      "Current progress: 55.56 %\n",
      "Current progress: 56.57 %\n",
      "Current progress: 57.58 %\n",
      "Current progress: 58.59 %\n",
      "Current progress: 59.6 %\n",
      "Current progress: 60.61 %\n",
      "Current progress: 61.62 %\n",
      "Current progress: 62.63 %\n",
      "Current progress: 63.64 %\n",
      "Current progress: 64.65 %\n",
      "Current progress: 65.66 %\n",
      "Current progress: 66.67 %\n",
      "Current progress: 67.68 %\n",
      "Current progress: 68.69 %\n",
      "Current progress: 69.7 %\n",
      "Current progress: 70.71 %\n",
      "Current progress: 71.72 %\n",
      "Current progress: 72.73 %\n",
      "Current progress: 73.74 %\n",
      "Current progress: 74.75 %\n",
      "Current progress: 75.76 %\n",
      "Current progress: 76.77 %\n",
      "Current progress: 77.78 %\n",
      "Current progress: 78.79 %\n",
      "Current progress: 79.8 %\n",
      "Current progress: 80.81 %\n",
      "Current progress: 81.82 %\n",
      "Current progress: 82.83 %\n",
      "Current progress: 83.84 %\n",
      "Current progress: 84.85 %\n",
      "Current progress: 85.86 %\n",
      "Current progress: 86.87 %\n",
      "Current progress: 87.88 %\n",
      "Current progress: 88.89 %\n",
      "Current progress: 89.9 %\n",
      "Current progress: 90.91 %\n",
      "Current progress: 91.92 %\n",
      "Current progress: 92.93 %\n",
      "Current progress: 93.94 %\n",
      "Current progress: 94.95 %\n",
      "Current progress: 95.96 %\n",
      "Current progress: 96.97 %\n",
      "Current progress: 97.98 %\n",
      "Current progress: 98.99 %\n",
      "Current progress: 100.0 %\n",
      "Done! \n"
     ]
    }
   ],
   "source": [
    "if DATA_DRIVEN_PREDICTION:\n",
    "    # Number for track id to be used\n",
    "    data_prepare_obj.track_id_range = 100\n",
    "    \n",
    "    data_prepare_obj.data_input = \"normalized_data\"\n",
    "    xTrain_data, xTest_data, yTrain_data, yTest_data = data_prepare_obj.get_test_train_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4af9d10",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "This concept of splitting the data into different parts is very important with regard to \"overfitting\". To get a bit of visual intuition, enjoy another video:\n",
    "[![](pictures_main/splitDataYouTube.png)](https://www.youtube.com/watch?v=77jUW9IoooM&list=PLZ6Q6guexAFO_gbbxjxrNVrFx0zgIbmyN&index=27&t=868s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146ee7bb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.3  Save Train, Test, Scalars in Pickle Files\n",
    "Here we use pickle library to save and load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f91a7490",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SAVE_PICKLE_FLAG = False\n",
    "OPEN_PICKLE_FLAG = False\n",
    "if DATA_DRIVEN_PREDICTION and SAVE_PICKLE_FLAG:\n",
    "    # Save the xTrain, xTest, yTrain, xTest in pickle format\n",
    "    data_prepare_obj.save_test_train_data_pickle()\n",
    "if DATA_DRIVEN_PREDICTION and OPEN_PICKLE_FLAG:\n",
    "    # Load the xTrain, xTest, yTrain, xTest from a pickle format\n",
    "    data_prepare_obj.load_test_train_data_pickle()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c521fcd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.0 Prediction Models\n",
    "\n",
    "In this section you can introduce prepared predictions models from the folder `/prediction_models`. Prepare your own model there and load it into this file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f590354",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 5.1 Constant Velocity Model\n",
    "Take a look into the python file to see what is going on there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83a04fdd",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# sys.path.append('prediction_models/constant_velocity/')\n",
    "# from const_vel import my_constant_vel_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6150e83",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 5.2 Constant Acceleration Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c828c1d2",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# sys.path.append('prediction_models/constant_acceleration/')\n",
    "# from const_acc import my_constant_acc_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67bfc04",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 5.3 Neural Network Models\n",
    "Take a look into the python file to see what is going on there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0dc31fae",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append('prediction_models/neural_networks/')\n",
    "from fcn_keras_model import FCN_keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f95bc7e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "31345/31345 [==============================] - 80s 2ms/step - loss: 0.0739 - val_loss: 0.0614\n",
      "Epoch 2/100\n",
      "31345/31345 [==============================] - 80s 3ms/step - loss: 0.0575 - val_loss: 0.0547\n",
      "Epoch 3/100\n",
      "31345/31345 [==============================] - 84s 3ms/step - loss: 0.0532 - val_loss: 0.0517\n",
      "Epoch 4/100\n",
      "31345/31345 [==============================] - 80s 3ms/step - loss: 0.0505 - val_loss: 0.0495\n",
      "Epoch 5/100\n",
      "31345/31345 [==============================] - 81s 3ms/step - loss: 0.0490 - val_loss: 0.0486\n",
      "Epoch 6/100\n",
      "31345/31345 [==============================] - 81s 3ms/step - loss: 0.0483 - val_loss: 0.0481\n",
      "Epoch 7/100\n",
      "31345/31345 [==============================] - 82s 3ms/step - loss: 0.0479 - val_loss: 0.0478\n",
      "Epoch 8/100\n",
      "31345/31345 [==============================] - 81s 3ms/step - loss: 0.0477 - val_loss: 0.0477\n",
      "Epoch 9/100\n",
      "31345/31345 [==============================] - 82s 3ms/step - loss: 0.0476 - val_loss: 0.0475\n",
      "Epoch 10/100\n",
      "31345/31345 [==============================] - 84s 3ms/step - loss: 0.0475 - val_loss: 0.0474\n",
      "Epoch 11/100\n",
      "31345/31345 [==============================] - 82s 3ms/step - loss: 0.0474 - val_loss: 0.0474\n",
      "Epoch 12/100\n",
      "31345/31345 [==============================] - 84s 3ms/step - loss: 0.0473 - val_loss: 0.0473\n",
      "Epoch 13/100\n",
      "31345/31345 [==============================] - 83s 3ms/step - loss: 0.0473 - val_loss: 0.0472\n",
      "Epoch 14/100\n",
      "31345/31345 [==============================] - 84s 3ms/step - loss: 0.0472 - val_loss: 0.0472\n",
      "Epoch 15/100\n",
      "31345/31345 [==============================] - 84s 3ms/step - loss: 0.0472 - val_loss: 0.0472\n",
      "Epoch 16/100\n",
      "31345/31345 [==============================] - 85s 3ms/step - loss: 0.0471 - val_loss: 0.0471\n",
      "Epoch 17/100\n",
      "31345/31345 [==============================] - 88s 3ms/step - loss: 0.0471 - val_loss: 0.0470\n",
      "Epoch 18/100\n",
      "31345/31345 [==============================] - 87s 3ms/step - loss: 0.0470 - val_loss: 0.0470\n",
      "Epoch 19/100\n",
      "31345/31345 [==============================] - 88s 3ms/step - loss: 0.0469 - val_loss: 0.0469\n",
      "Epoch 20/100\n",
      "31345/31345 [==============================] - 91s 3ms/step - loss: 0.0468 - val_loss: 0.0468\n",
      "Epoch 21/100\n",
      "31345/31345 [==============================] - 99s 3ms/step - loss: 0.0467 - val_loss: 0.0467\n",
      "Epoch 22/100\n",
      "31345/31345 [==============================] - 90s 3ms/step - loss: 0.0466 - val_loss: 0.0466\n",
      "Epoch 23/100\n",
      "31345/31345 [==============================] - 91s 3ms/step - loss: 0.0465 - val_loss: 0.0464\n",
      "Epoch 24/100\n",
      "31345/31345 [==============================] - 91s 3ms/step - loss: 0.0464 - val_loss: 0.0463\n",
      "Epoch 25/100\n",
      "31345/31345 [==============================] - 92s 3ms/step - loss: 0.0463 - val_loss: 0.0463\n",
      "Epoch 26/100\n",
      "31345/31345 [==============================] - 92s 3ms/step - loss: 0.0462 - val_loss: 0.0462\n",
      "Epoch 27/100\n",
      "31345/31345 [==============================] - 91s 3ms/step - loss: 0.0461 - val_loss: 0.0461\n",
      "Epoch 28/100\n",
      "31345/31345 [==============================] - 96s 3ms/step - loss: 0.0461 - val_loss: 0.0461\n",
      "Epoch 29/100\n",
      "31345/31345 [==============================] - 92s 3ms/step - loss: 0.0460 - val_loss: 0.0460\n",
      "Epoch 30/100\n",
      "31345/31345 [==============================] - 93s 3ms/step - loss: 0.0460 - val_loss: 0.0460\n",
      "Epoch 31/100\n",
      "31345/31345 [==============================] - 97s 3ms/step - loss: 0.0460 - val_loss: 0.0460\n",
      "Epoch 32/100\n",
      "31345/31345 [==============================] - 94s 3ms/step - loss: 0.0460 - val_loss: 0.0460\n",
      "Epoch 33/100\n",
      "31345/31345 [==============================] - 98s 3ms/step - loss: 0.0459 - val_loss: 0.0459\n",
      "Epoch 34/100\n",
      "31345/31345 [==============================] - 102s 3ms/step - loss: 0.0459 - val_loss: 0.0459\n",
      "Epoch 35/100\n",
      "31345/31345 [==============================] - 101s 3ms/step - loss: 0.0459 - val_loss: 0.0460\n",
      "Epoch 36/100\n",
      "31345/31345 [==============================] - 104s 3ms/step - loss: 0.0459 - val_loss: 0.0459\n",
      "Epoch 37/100\n",
      "31345/31345 [==============================] - 101s 3ms/step - loss: 0.0459 - val_loss: 0.0459\n",
      "Epoch 38/100\n",
      "31345/31345 [==============================] - 106s 3ms/step - loss: 0.0459 - val_loss: 0.0459\n",
      "Epoch 39/100\n",
      "31345/31345 [==============================] - 109s 3ms/step - loss: 0.0458 - val_loss: 0.0459\n",
      "Epoch 40/100\n",
      "31345/31345 [==============================] - 104s 3ms/step - loss: 0.0458 - val_loss: 0.0458\n",
      "Epoch 41/100\n",
      "31345/31345 [==============================] - 105s 3ms/step - loss: 0.0458 - val_loss: 0.0458\n",
      "Epoch 42/100\n",
      "31345/31345 [==============================] - 106s 3ms/step - loss: 0.0458 - val_loss: 0.0458\n",
      "Epoch 43/100\n",
      "31345/31345 [==============================] - 106s 3ms/step - loss: 0.0458 - val_loss: 0.0458\n",
      "Epoch 44/100\n",
      "31345/31345 [==============================] - 108s 3ms/step - loss: 0.0458 - val_loss: 0.0458\n",
      "Epoch 45/100\n",
      "31345/31345 [==============================] - 106s 3ms/step - loss: 0.0458 - val_loss: 0.0458\n",
      "Epoch 46/100\n",
      "31345/31345 [==============================] - 111s 4ms/step - loss: 0.0458 - val_loss: 0.0458\n",
      "Epoch 47/100\n",
      "31345/31345 [==============================] - 113s 4ms/step - loss: 0.0458 - val_loss: 0.0458\n",
      "Epoch 48/100\n",
      "31345/31345 [==============================] - 117s 4ms/step - loss: 0.0458 - val_loss: 0.0458\n",
      "Epoch 49/100\n",
      "31345/31345 [==============================] - 115s 4ms/step - loss: 0.0457 - val_loss: 0.0458\n",
      "Epoch 50/100\n",
      "31345/31345 [==============================] - 114s 4ms/step - loss: 0.0457 - val_loss: 0.0458\n",
      "Epoch 51/100\n",
      "31345/31345 [==============================] - 117s 4ms/step - loss: 0.0457 - val_loss: 0.0458\n",
      "Epoch 52/100\n",
      "31345/31345 [==============================] - 117s 4ms/step - loss: 0.0457 - val_loss: 0.0458\n",
      "Epoch 53/100\n",
      "31345/31345 [==============================] - 119s 4ms/step - loss: 0.0457 - val_loss: 0.0458\n",
      "Epoch 54/100\n",
      "31345/31345 [==============================] - 127s 4ms/step - loss: 0.0457 - val_loss: 0.0457\n",
      "Epoch 55/100\n",
      "31345/31345 [==============================] - 129s 4ms/step - loss: 0.0457 - val_loss: 0.0457\n",
      "Epoch 56/100\n",
      "31345/31345 [==============================] - 125s 4ms/step - loss: 0.0457 - val_loss: 0.0458\n",
      "Epoch 57/100\n",
      "31345/31345 [==============================] - 126s 4ms/step - loss: 0.0457 - val_loss: 0.0458\n",
      "Epoch 58/100\n",
      "31345/31345 [==============================] - 126s 4ms/step - loss: 0.0457 - val_loss: 0.0457\n",
      "Epoch 59/100\n",
      "31345/31345 [==============================] - 130s 4ms/step - loss: 0.0457 - val_loss: 0.0457\n",
      "Epoch 60/100\n",
      "31345/31345 [==============================] - 131s 4ms/step - loss: 0.0457 - val_loss: 0.0457\n",
      "Epoch 61/100\n",
      "31345/31345 [==============================] - 128s 4ms/step - loss: 0.0457 - val_loss: 0.0457\n",
      "Epoch 62/100\n",
      "31345/31345 [==============================] - 128s 4ms/step - loss: 0.0457 - val_loss: 0.0457\n",
      "Epoch 63/100\n",
      "31345/31345 [==============================] - 133s 4ms/step - loss: 0.0456 - val_loss: 0.0457\n",
      "Epoch 64/100\n",
      "31345/31345 [==============================] - 137s 4ms/step - loss: 0.0456 - val_loss: 0.0456\n",
      "Epoch 65/100\n",
      "31345/31345 [==============================] - 139s 4ms/step - loss: 0.0456 - val_loss: 0.0457\n",
      "Epoch 66/100\n",
      "31345/31345 [==============================] - 141s 4ms/step - loss: 0.0456 - val_loss: 0.0456\n",
      "Epoch 67/100\n",
      "31345/31345 [==============================] - 144s 5ms/step - loss: 0.0456 - val_loss: 0.0456\n",
      "Epoch 68/100\n",
      "31345/31345 [==============================] - 143s 5ms/step - loss: 0.0456 - val_loss: 0.0456\n",
      "Epoch 69/100\n",
      "31345/31345 [==============================] - 147s 5ms/step - loss: 0.0456 - val_loss: 0.0456\n",
      "Epoch 70/100\n",
      "31345/31345 [==============================] - 147s 5ms/step - loss: 0.0456 - val_loss: 0.0456\n",
      "Epoch 71/100\n",
      "31345/31345 [==============================] - 149s 5ms/step - loss: 0.0456 - val_loss: 0.0456\n",
      "Epoch 72/100\n",
      "31345/31345 [==============================] - 151s 5ms/step - loss: 0.0455 - val_loss: 0.0456\n",
      "Epoch 73/100\n",
      "31345/31345 [==============================] - 152s 5ms/step - loss: 0.0455 - val_loss: 0.0456\n",
      "Epoch 74/100\n",
      "31345/31345 [==============================] - 153s 5ms/step - loss: 0.0455 - val_loss: 0.0455\n",
      "Epoch 75/100\n",
      "31345/31345 [==============================] - 152s 5ms/step - loss: 0.0455 - val_loss: 0.0456\n",
      "Epoch 76/100\n",
      "31345/31345 [==============================] - 148s 5ms/step - loss: 0.0455 - val_loss: 0.0456\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31345/31345 [==============================] - 151s 5ms/step - loss: 0.0455 - val_loss: 0.0455\n",
      "Epoch 78/100\n",
      "31345/31345 [==============================] - 175s 6ms/step - loss: 0.0455 - val_loss: 0.0455\n",
      "Epoch 79/100\n",
      "31345/31345 [==============================] - 200s 6ms/step - loss: 0.0455 - val_loss: 0.0456\n",
      "Epoch 80/100\n",
      "31345/31345 [==============================] - 180s 6ms/step - loss: 0.0454 - val_loss: 0.0456\n",
      "Epoch 81/100\n",
      "31345/31345 [==============================] - 210s 7ms/step - loss: 0.0454 - val_loss: 0.0455\n",
      "Epoch 82/100\n",
      "31345/31345 [==============================] - 211s 7ms/step - loss: 0.0454 - val_loss: 0.0455\n",
      "Epoch 83/100\n",
      "31345/31345 [==============================] - 182s 6ms/step - loss: 0.0454 - val_loss: 0.0454\n",
      "Epoch 84/100\n",
      "31345/31345 [==============================] - 173s 6ms/step - loss: 0.0454 - val_loss: 0.0455\n",
      "Epoch 85/100\n",
      "31345/31345 [==============================] - 172s 5ms/step - loss: 0.0454 - val_loss: 0.0454\n",
      "Epoch 86/100\n",
      "31345/31345 [==============================] - 179s 6ms/step - loss: 0.0454 - val_loss: 0.0454\n",
      "Epoch 87/100\n",
      "31345/31345 [==============================] - 185s 6ms/step - loss: 0.0454 - val_loss: 0.0454\n",
      "Epoch 88/100\n",
      "31345/31345 [==============================] - 196s 6ms/step - loss: 0.0454 - val_loss: 0.0454\n",
      "Epoch 89/100\n",
      "31345/31345 [==============================] - 204s 6ms/step - loss: 0.0454 - val_loss: 0.0454\n",
      "Epoch 90/100\n",
      "31345/31345 [==============================] - 209s 7ms/step - loss: 0.0454 - val_loss: 0.0454\n",
      "Epoch 91/100\n",
      "31345/31345 [==============================] - 209s 7ms/step - loss: 0.0453 - val_loss: 0.0454\n",
      "Epoch 92/100\n",
      "31345/31345 [==============================] - 201s 6ms/step - loss: 0.0453 - val_loss: 0.0454\n",
      "Epoch 93/100\n",
      "31345/31345 [==============================] - 194s 6ms/step - loss: 0.0453 - val_loss: 0.0453\n",
      "Epoch 94/100\n",
      "31345/31345 [==============================] - 211s 7ms/step - loss: 0.0453 - val_loss: 0.0453\n",
      "Epoch 95/100\n",
      "31345/31345 [==============================] - 201s 6ms/step - loss: 0.0453 - val_loss: 0.0453\n",
      "Epoch 96/100\n",
      "31345/31345 [==============================] - 196s 6ms/step - loss: 0.0453 - val_loss: 0.0453\n",
      "Epoch 97/100\n",
      "31345/31345 [==============================] - 200s 6ms/step - loss: 0.0453 - val_loss: 0.0454\n",
      "Epoch 98/100\n",
      "31345/31345 [==============================] - 195s 6ms/step - loss: 0.0453 - val_loss: 0.0453\n",
      "Epoch 99/100\n",
      "31345/31345 [==============================] - 201s 6ms/step - loss: 0.0453 - val_loss: 0.0453\n",
      "Epoch 100/100\n",
      "31345/31345 [==============================] - 205s 7ms/step - loss: 0.0453 - val_loss: 0.0453\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# if DATA_DRIVEN_PREDICTION:\n",
    "#     FCN_model =  FCN_keras_model(xTrain_data, xTest_data, yTrain_data, yTest_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "def74e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout, Activation\n",
    "from keras.optimizers import SGD \n",
    "import numpy as np\n",
    "\n",
    "def build_model(hp):\n",
    "    #flatten the input and output\n",
    "    n_input = 20 * 4\n",
    "    n_output = 3 * 15\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(\n",
    "        hp.Int('units', min_value=100, max_value=500, step=10),\n",
    "        activation='relu', input_shape=(n_input,))) #\n",
    "    model.add(Dense(\n",
    "        hp.Int('units', min_value=100, max_value=300, step=10),\n",
    "        activation='relu'))\n",
    "    model.add(Dense(\n",
    "        hp.Int('units', min_value=50, max_value=200, step=10),\n",
    "        activation='relu'))\n",
    "    model.add(Dense(\n",
    "        hp.Int('units', min_value=10, max_value=50, step=10),\n",
    "        activation='relu'))\n",
    "    model.add(Dense(n_output))\n",
    "    model.compile(\n",
    "        optimizer=SGD(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log'),\n",
    "                      hp.Float('momentum', 1e-4, 1e-2, sampling='log'),\n",
    "                      nesterov=True),\n",
    "        loss='mse')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc0a248a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 15)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(yTrain_data)[1], np.shape(yTrain_data)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eee01a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project .\\untitled_project\\oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from .\\untitled_project\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc0bf7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# flatten the input and output\n",
    "n_input = np.shape(xTrain_data)[1] * np.shape(xTrain_data)[2]\n",
    "xTrain1 = np.reshape(xTrain_data, (np.shape(xTrain_data)[0], n_input))\n",
    "n_output = np.shape(yTrain_data)[1] * np.shape(yTrain_data)[2]\n",
    "yTrain1 = np.reshape(yTrain_data, (np.shape(yTrain_data)[0], n_output))\n",
    "\n",
    "xTest1 = np.reshape(xTest_data, (np.shape(xTest_data)[0], n_input))\n",
    "yTest1 = np.reshape(yTest_data, (np.shape(yTest_data)[0], n_output))\n",
    "tuner.search(xTrain1, yTrain1,\n",
    "             validation_data=(xTest1, yTest1),\n",
    "             epochs=50) #callbacks=[tf.keras.callbacks.EarlyStopping(patience=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b238fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0545b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_tuner.engine.hyperparameters.HyperParameters at 0x18f56ba3c40>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyperparameters = tuner.get_best_hyperparameters(1)[0]\n",
    "best_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2a41fa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 280)               22680     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 280)               78680     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 280)               78680     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 280)               78680     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 45)                12645     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271,365\n",
      "Trainable params: 271,365\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8e40060",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'best_FCN_model.h5'\n",
    "best_model.save(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d2a5d6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 6.0 Prediction Testing\n",
    "\n",
    "To see how well we did we need to use our model to make predictions. At the end of this section we want to write the data into an excel sheet. This excel sheet is needed to evaluate your predictions in a standardized fashion (important for the competition).\n",
    "Now we need to generate predictions and compare it to the ground truth - what actually happened."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dd5855",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6.1 Generate Testset Data\n",
    "First we produce some predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e212b584",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6.1.1 For Physics Based Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62bef48e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if PHYSICS_BASED_PREDICTION:\n",
    "    recording_id_sel = ['27']\n",
    "    data_sel_id = 0 # If there are multiple recordings added\n",
    "    \n",
    "    # Initialize data Grabber Object\n",
    "    test_data_obj = dataGrabber(dataset_path)\n",
    "\n",
    "    test_data_obj.recording_id = recording_id_sel\n",
    "    test_data_obj.read_csv_with_recordingID()\n",
    "\n",
    "    test_track_data = test_data_obj.get_tracks_data()\n",
    "    test_track_meta_data = test_data_obj.get_tracksMeta_data() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d94fe-324b-43c7-bfc6-129ed7a38d01",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### Downsampling Data to Match Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b4d58cd",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if PHYSICS_BASED_PREDICTION:\n",
    "    test_pre_process_obj = preProcess()\n",
    "    test_pre_process_obj.tracks_data = test_track_data\n",
    "    test_pre_process_obj.tracks_meta_data = test_track_meta_data\n",
    "    test_pre_process_obj.recording_ids = test_data_obj.recording_id\n",
    "    test_pre_process_obj.data_len = len(test_track_data)\n",
    "    \n",
    "    test_pre_process_obj.frames_skipped = 5\n",
    "    new_sampling_rate = 0.2\n",
    "    test_pre_process_obj.new_sampling_rate = new_sampling_rate\n",
    "    test_track_data_downsampled, test_tracks_meta_data = test_pre_process_obj.get_down_sampled_data()\n",
    "    #test_pre_process_obj.tracks_data = test_track_data_downsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c84425",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Concatinating Dataframes and Sorting by \"frame\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0649dfab",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if PHYSICS_BASED_PREDICTION:\n",
    "    sel_id = 0 # if multiple recording ids are selected -> index value of the array has to be given\n",
    "        \n",
    "    test_track_data_sel = track_data_downsampled\n",
    "    test_track_meta_data_sel = test_track_meta_data\n",
    "\n",
    "    # Reorder Tracks File by \"frame\"\n",
    "    test_track_data_sel = test_track_data_sel.sort_values([\"frame\"], axis = 0, ascending = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb2b106",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6.1.2 For Data Driven Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4dfb74c7-8ef9-4069-b697-3c9ac81f160d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if DATA_DRIVEN_PREDICTION:   \n",
    "    recording_id_sel = ['28']\n",
    "    data_sel_id = 0 # If there are multiple recordings added\n",
    "    \n",
    "    # Initialize data Grabber Object\n",
    "    test_data_obj = dataGrabber(dataset_path)\n",
    "\n",
    "    test_data_obj.recording_id = recording_id_sel\n",
    "    test_data_obj.read_csv_with_recordingID()\n",
    "\n",
    "    test_track_data = test_data_obj.get_tracks_data()\n",
    "    test_track_meta_data = test_data_obj.get_tracksMeta_data()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e2f348",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### Preprocessing and Downsampling Data to Match Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f031228c-a865-482f-b7c0-8817820618aa",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if DATA_DRIVEN_PREDICTION:   \n",
    "    test_pre_process_obj = preProcess()\n",
    "    test_pre_process_obj.tracks_data = test_track_data\n",
    "    test_pre_process_obj.tracks_meta_data = test_track_meta_data\n",
    "    test_pre_process_obj.recording_ids = test_data_obj.recording_id\n",
    "    test_pre_process_obj.data_len = len(test_track_data)\n",
    "    \n",
    "    test_pre_process_obj.frames_skipped = 5\n",
    "    test_track_data_downsampled, test_tracks_meta_data = test_pre_process_obj.get_down_sampled_data()\n",
    "    test_pre_process_obj.tracks_data = test_track_data_downsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394ae744-5d19-4bc4-a907-467e44fc031f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e355a67-e239-4bd7-966b-818ba3e63cd4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if DATA_DRIVEN_PREDICTION:   \n",
    "    # Gets the tracks data normalized\n",
    "    test_tracks_data_norm, min_max_scalar_list = test_pre_process_obj.normalize_data()\n",
    "     # Resetting dropped frames index\n",
    "    test_tracks_data_norm = test_tracks_data_norm.reset_index(drop=True)\n",
    "    \n",
    "    # Saving Normalized Data\n",
    "    test_data_prepare_obj = dataPrepare()\n",
    "    test_data_prepare_obj.tracks_data_norm = test_tracks_data_norm\n",
    "    test_data_prepare_obj.tracksMeta_data = test_tracks_meta_data\n",
    "    test_data_prepare_obj.data_len = len(test_tracks_data_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d070ae-8bdf-47cc-9236-ba1c18fa11f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8b16c8e-3ff9-4101-a1d5-c08956864905",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This might take a while!\n",
      "Current progress: 0.0 %\n",
      "Current progress: 11.11 %\n",
      "Current progress: 22.22 %\n",
      "Current progress: 33.33 %\n",
      "Current progress: 44.44 %\n",
      "Current progress: 55.56 %\n",
      "Current progress: 66.67 %\n",
      "Current progress: 77.78 %\n",
      "Current progress: 88.89 %\n",
      "Current progress: 100.0 %\n",
      "Done! \n"
     ]
    }
   ],
   "source": [
    "if DATA_DRIVEN_PREDICTION:\n",
    "    # Number for track id to be used\n",
    "    test_data_prepare_obj.track_id_range = 10  \n",
    "    \n",
    "    # Gets the tracks data normalized and its ID\n",
    "    test_data_prepare_obj.data_input = \"normalized_data\"\n",
    "    t_norm_Ids, t_in_norm, t_out_norm = test_data_prepare_obj.data_stacking()\n",
    "    # Predict the output\n",
    "    n_input = np.shape(t_in_norm)[1] * np.shape(t_in_norm)[2]\n",
    "    t_in_norm_reshaped = np.reshape(t_in_norm, (np.shape(t_in_norm)[0], n_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97db666c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6.2 Collect Ground Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558ec71d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 6.2.1 For Physics Based Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6a3ac45",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append('evaluation/')\n",
    "from physics_based_pred_evaluator import physicsBasedEvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c926eba0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if PHYSICS_BASED_PREDICTION: \n",
    "    phy_eval_obj = physicsBasedEvaluation()\n",
    "    \n",
    "    phy_eval_obj.selected_data = test_track_data_sel\n",
    "\n",
    "     # Setting Other Parameters        \n",
    "    phy_eval_obj.max_num_frames = int(test_track_data_sel.max()[\"frame\"])\n",
    "    phy_eval_obj.recording_id = recording_id_sel[sel_id]\n",
    "    \n",
    "    phy_eval_obj.pred_horizon = 15\n",
    "    phy_eval_obj.frame_range = 100\n",
    "    \n",
    "    # Should be same as what used during downsampling => 5\n",
    "    phy_eval_obj.frames_skipped = pre_process_obj.frames_skipped \n",
    "    \n",
    "    track_data_sampled = list()\n",
    "    test_track_data_sel = test_track_data_sel.sort_values([\"frame\", \"trackId\"], axis = 0, ascending = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583a9323",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Get Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f33f4545",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if PHYSICS_BASED_PREDICTION:\n",
    "    ground_truth, track_id_counter = phy_eval_obj.get_ground_truth()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ef8380",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Get Predicted Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "77ff7c48",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if PHYSICS_BASED_PREDICTION:\n",
    "    const_vel_model_prediction = my_constant_vel_model(test_track_data_sel, phy_eval_obj.pred_horizon, new_sampling_rate, phy_eval_obj.frame_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88900d0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Storing Predicted Values and Ground Truth into the Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7dd3283e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if PHYSICS_BASED_PREDICTION:\n",
    "    phy_eval_obj.predicted_data = const_vel_model_prediction\n",
    "    phy_eval_obj.ground_truth_data = ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db44f25",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "Create Evaluation Workbook and Add Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0353268",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if PHYSICS_BASED_PREDICTION:\n",
    "    work_book_filename = 'constant_velocity_prediction_result.xlsx' \n",
    "elif DATA_DRIVEN_PREDICTION:\n",
    "    work_book_filename = 'NN_velocity_prediction_result.xlsx' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19f19dd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Delete the File if Exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a3bef34c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if PHYSICS_BASED_PREDICTION:\n",
    "    if os.path.exists(work_book_filename):\n",
    "        os.remove(work_book_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afb8d79-7de5-4309-896a-6dd4f4cdf37e",
   "metadata": {},
   "source": [
    "Write To Workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c038b494",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if PHYSICS_BASED_PREDICTION:\n",
    "    phy_eval_obj.wb_filename = work_book_filename\n",
    "    phy_eval_obj.write_to_workbook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c80c63",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 6.2.2 For Data-Driven Prediction Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ebc4f768",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append('evaluation/')\n",
    "from data_driven_pred_evaluator import dataDrivenEvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "addfc7b3",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if DATA_DRIVEN_PREDICTION: \n",
    "    data_eval_obj = dataDrivenEvaluation()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29fc0d8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Get Ground Truth Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b7b173c5-0514-498d-9126-6dc93924d17e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This might take a while!\n",
      "Current progress: 0.0 %\n",
      "Current progress: 11.11 %\n",
      "Current progress: 22.22 %\n",
      "Current progress: 33.33 %\n",
      "Current progress: 44.44 %\n",
      "Current progress: 55.56 %\n",
      "Current progress: 66.67 %\n",
      "Current progress: 77.78 %\n",
      "Current progress: 88.89 %\n",
      "Current progress: 100.0 %\n",
      "Done! \n"
     ]
    }
   ],
   "source": [
    "if DATA_DRIVEN_PREDICTION:\n",
    "    # Resetting dropped frames index\n",
    "    test_track_data_downsampled = test_track_data_downsampled.reset_index(drop=True)\n",
    "    ground_truth_prepare_obj = dataPrepare()\n",
    "    ground_truth_prepare_obj.data_input = \"raw_data\"\n",
    "    ground_truth_prepare_obj.track_id_range = 10\n",
    "    ground_truth_prepare_obj.tracksMeta_data = test_tracks_meta_data\n",
    "    ground_truth_prepare_obj.tracks_data_norm = test_tracks_data_norm\n",
    "    ground_truth_prepare_obj.tracks_data = test_track_data_downsampled\n",
    "    ground_truth_prepare_obj.data_len = len(test_track_data_downsampled) \n",
    "    #ground_truth_prepare_obj.num_predict = 15\n",
    "    t_raw_Ids, t_in_raw, t_out_raw = ground_truth_prepare_obj.data_stacking()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bbc450-7e4d-4cd7-b5cb-d81714f12520",
   "metadata": {},
   "source": [
    "Copy ground truth data to data driven evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8d3d5ce5-b1bc-4d30-aedd-2247a430db2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_DRIVEN_PREDICTION:\n",
    "    data_eval_obj.t_raw_Ids = t_raw_Ids\n",
    "    data_eval_obj.t_in_raw = t_in_raw\n",
    "    data_eval_obj.t_out_raw = t_out_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b15c41-b058-456b-a4c5-d4c3a52e4967",
   "metadata": {},
   "source": [
    "Get Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f2e4ab5d-cf2e-4077-a11e-87ddcaaa42dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_DRIVEN_PREDICTION:\n",
    "    xCenter_gt, yCenter_gt, heading_gt = data_eval_obj.get_ground_truth()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00420ed6-beb8-4b9e-9771-05d0f74667d0",
   "metadata": {},
   "source": [
    "Get Predicted Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b32d89-4e45-42ad-859b-5920f11a0289",
   "metadata": {},
   "source": [
    "Use Predict Function for the Model Created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "47a99abc-12d6-46a1-947d-3496de21faa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "model1 = keras.models.load_model('best_FCN_model.h5')\n",
    "if DATA_DRIVEN_PREDICTION:\n",
    "    yhat = model1.predict(t_in_norm_reshaped, verbose=0)\n",
    "    \n",
    "    # Save Predicted Data into the Evaluator\n",
    "    data_eval_obj.y_hat = yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f86d5ba0-f3e5-42fa-aa3f-aafad56d02b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_DRIVEN_PREDICTION:\n",
    "    # Set Paramters\n",
    "    data_eval_obj.min_max_scalar_list = min_max_scalar_list\n",
    "    \n",
    "    # Get Prediction\n",
    "    xCenter_prediction, yCenter_prediction, heading_prediction = data_eval_obj.get_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c2eb9c-a12f-4b89-b9c0-bd4dcfd259ad",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Delete the File if Exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b19582aa-5fdb-48b5-870b-f1acce5b1548",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DATA_DRIVEN_PREDICTION:\n",
    "    if os.path.exists(work_book_filename):\n",
    "        os.remove(work_book_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407accf1-e208-4f51-859d-db26b509d1e4",
   "metadata": {},
   "source": [
    "Write To Workbook (Excel-Sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "732db8d0-0c47-44e4-ad12-882ad7701df8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if DATA_DRIVEN_PREDICTION:\n",
    "    data_eval_obj.wb_filename = work_book_filename\n",
    "    data_eval_obj.write_to_workbook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e63ad7-2c0c-4d79-aaaa-2629721f3af0",
   "metadata": {},
   "source": [
    "## 7.0 Evaluation\n",
    "Now it's the moment of truth!! We load the data now from the excel sheet and calculate three error metrics:\n",
    "1. **Average Displacement Error (ADE):** \n",
    "Average Displacement Error (ADE): ADE refers to the mean square error (MSE) over all estimated points of every trajectory and the true points.\n",
    "$$\n",
    "\\text{ADE} = \\frac\n",
    "{\\sum_{i=1}^{n}\\sum_{t=T_{Frame}}^{T_{pred}} \\quad  \\big[(\\hat{x}_i^t - x_i^t)^2 + (\\hat{y}_i^t - y_i^t)^2 \\big]}\n",
    "{n(T_{pred}-(T_{Frame}+1))}\n",
    "$$\n",
    "\n",
    "2. **Final displacement error (FDE):** \n",
    "FDE means the distance between the predicted final destination and the true final destination at the $T_{pred}$ time.\n",
    "$$\n",
    "\\text{FDE} = \\frac\n",
    "{\\sum_{i=1}^n \\sqrt{\\big( \\, \\hat{x}_i^{T_{pred}} - x_i^{T_{pred}} \\, \\,\\big)^2  + \\big( \\, \\hat{y}_i^{T_{pred}} - y_i^{T_{pred}} \\, \\,\\big)^2 }}\n",
    "{n}\n",
    "$$ \n",
    "\n",
    "3. **Average Absolute Heading Error (AHE):** \n",
    "This is a bit like ADE but we take the 1-norm of the error and we only consider the heading prediction here.\n",
    "$$\n",
    "\\text{AHE} = \\frac\n",
    "{\\sum_{i=1}^{n}\\sum_{t=T_{Frame}}^{T_{pred}} \\quad \\big| \\hat{y}_i^t - y_i^t \\big| }\n",
    "{n(T_{pred}-(T_{Frame}+1))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6b02d084-29be-4025-9b06-4ce65aac47a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation_matrix import evaluationMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a161ff03-7bc7-4a19-b0b1-69e1c4d9da86",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PHYSICS_BASED_PREDICTION:\n",
    "    eval_obj = evaluationMatrix(work_book_filename, phy_eval_obj.pred_horizon)  \n",
    "elif DATA_DRIVEN_PREDICTION:\n",
    "    eval_obj = evaluationMatrix(work_book_filename, data_eval_obj.n_predict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f080f04a-b29a-494e-ab2b-a78edcd5a5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average displacement error is 1.668 m\n",
      "The average final displacement error is 1.806 m\n",
      "The average absolute heading error is 8.11 degrees\n"
     ]
    }
   ],
   "source": [
    "ade_value, fde_value, ahe_val = eval_obj.get_fde_ade_ahe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bf8fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f104f96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6cc0136a1d0756f8532771372b10b5131b73f8263c41cdf1b1974679577f2060"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
